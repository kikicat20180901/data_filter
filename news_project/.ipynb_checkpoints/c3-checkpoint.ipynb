{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c9cdf21-5bf9-4ca5-8c47-a4ced4712837",
   "metadata": {},
   "source": [
    "您提出了一个非常有趣且复杂的任务。我会从几个角度分析当前实现方法，并提出一些优化建议。\n",
    "\n",
    "1. 实现效率\n",
    "\n",
    "当前方法的主要效率瓶颈:\n",
    "- 多次调用LLM API (提炼关键词、提取天数、生成摘要)\n",
    "- 串行处理每条新闻的详细内容\n",
    "- 可能存在重复或不必要的API调用\n",
    "\n",
    "优化建议:\n",
    "a) 合并LLM调用:\n",
    "将关键词提取和天数提取合并为一次LLM调用，可以减少API请求次数。\n",
    "\n",
    "```python\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"\n",
    "从以下问题中提取一个关键词和所需的天数信息:\n",
    "问题: {question}\n",
    "\n",
    "输出格式:\n",
    "关键词: <关键词>\n",
    "天数: <天数>\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "result = llm_chain.run(question)\n",
    "keyword, days = parse_llm_output(result)  # 解析LLM输出\n",
    "```\n",
    "\n",
    "b) 并行处理新闻内容:\n",
    "使用异步编程或多线程来并行获取和处理新闻内容。\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "import aiohttp\n",
    "\n",
    "async def fetch_news_content(session, story_id):\n",
    "    url = f\"https://api.refinitiv.com/news/{story_id}\"\n",
    "    async with session.get(url) as response:\n",
    "        return await response.text()\n",
    "\n",
    "async def fetch_all_news(story_ids):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [fetch_news_content(session, story_id) for story_id in story_ids]\n",
    "        return await asyncio.gather(*tasks)\n",
    "\n",
    "# 使用方法\n",
    "story_ids = [...]  # 从第一次API调用获取的story_ids列表\n",
    "news_contents = asyncio.run(fetch_all_news(story_ids))\n",
    "```\n",
    "\n",
    "2. 实现成本\n",
    "\n",
    "主要成本来源:\n",
    "- LLM API调用费用\n",
    "- Refinitiv API调用费用\n",
    "\n",
    "优化建议:\n",
    "a) 缓存机制:\n",
    "实现一个缓存系统来存储常见问题的结果，减少重复API调用。\n",
    "\n",
    "```python\n",
    "import redis\n",
    "import json\n",
    "\n",
    "redis_client = redis.Redis(host='localhost', port=6379, db=0)\n",
    "\n",
    "def get_cached_result(question):\n",
    "    return redis_client.get(question)\n",
    "\n",
    "def set_cached_result(question, result):\n",
    "    redis_client.setex(question, 3600, json.dumps(result))  # 缓存1小时\n",
    "\n",
    "# 使用方法\n",
    "cached_result = get_cached_result(question)\n",
    "if cached_result:\n",
    "    return json.loads(cached_result)\n",
    "else:\n",
    "    # 执行正常流程\n",
    "    result = process_question(question)\n",
    "    set_cached_result(question, result)\n",
    "    return result\n",
    "```\n",
    "\n",
    "b) 智能批处理:\n",
    "根据用户查询模式，预测可能的后续查询，批量获取和处理数据。\n",
    "\n",
    "3. 运行速度\n",
    "\n",
    "主要瓶颈:\n",
    "- API调用延迟\n",
    "- 串行处理\n",
    "\n",
    "优化建议:\n",
    "a) 使用异步编程:\n",
    "除了前面提到的并行处理新闻内容，还可以将整个流程改为异步。\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "from aiohttp import ClientSession\n",
    "\n",
    "async def process_question(question):\n",
    "    async with ClientSession() as session:\n",
    "        keyword, days = await extract_info(question)\n",
    "        headlines = await fetch_headlines(session, keyword, days)\n",
    "        news_contents = await fetch_all_news(session, headlines)\n",
    "        summary = await generate_summary(news_contents)\n",
    "    return summary\n",
    "\n",
    "# 使用方法\n",
    "summary = asyncio.run(process_question(question))\n",
    "```\n",
    "\n",
    "b) 流式处理:\n",
    "不等待所有新闻内容都获取完毕，而是在获取一定数量后就开始生成摘要。\n",
    "\n",
    "4. 可扩展性\n",
    "\n",
    "当前实现可能面临的挑战:\n",
    "- 难以处理大量并发请求\n",
    "- 难以适应新的数据源或LLM模型\n",
    "\n",
    "优化建议:\n",
    "a) 微服务架构:\n",
    "将整个流程拆分为多个独立的微服务，每个服务负责特定的功能。\n",
    "\n",
    "b) 消息队列:\n",
    "使用消息队列来处理请求，提高系统的可伸缩性和容错性。\n",
    "\n",
    "```python\n",
    "from celery import Celery\n",
    "\n",
    "app = Celery('tasks', broker='redis://localhost:6379/0')\n",
    "\n",
    "@app.task\n",
    "def process_question(question):\n",
    "    # 实现处理逻辑\n",
    "    pass\n",
    "\n",
    "# 使用方法\n",
    "result = process_question.delay(question)\n",
    "summary = result.get()  # 获取结果\n",
    "```\n",
    "\n",
    "c) 插件化设计:\n",
    "设计一个插件系统，允许轻松添加新的数据源或LLM模型。\n",
    "\n",
    "```python\n",
    "class DataSourcePlugin:\n",
    "    def fetch_data(self, keyword, days):\n",
    "        pass\n",
    "\n",
    "class LLMPlugin:\n",
    "    def generate_summary(self, content):\n",
    "        pass\n",
    "\n",
    "class PluginManager:\n",
    "    def __init__(self):\n",
    "        self.data_sources = {}\n",
    "        self.llm_models = {}\n",
    "\n",
    "    def register_data_source(self, name, plugin):\n",
    "        self.data_sources[name] = plugin\n",
    "\n",
    "    def register_llm_model(self, name, plugin):\n",
    "        self.llm_models[name] = plugin\n",
    "\n",
    "# 使用方法\n",
    "manager = PluginManager()\n",
    "manager.register_data_source(\"refinitiv\", RefinitivPlugin())\n",
    "manager.register_llm_model(\"gpt3\", GPT3Plugin())\n",
    "```\n",
    "\n",
    "这些优化建议涵盖了多个方面，包括提高效率、降低成本、提升速度和增强可扩展性。具体实施时，您可能需要根据实际需求和资源情况进行取舍和调整。此外，还建议进行性能测试和监控，以确保优化措施确实带来了预期的改进。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bdb61b-f160-47e8-8d69-099cbb00f2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python tinyrag",
   "language": "python",
   "name": "tinyrag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
